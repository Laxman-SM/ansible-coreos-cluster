- debug: var=vpc_id

# set facts with file content for cloud-config
# gaia CA crt content
- set_fact:
    gaia_ca_crt_file: "keys/{{ ec2_region }}/{{ gaia_ca_file }}.crt"
- stat:
    path: "{{ gaia_ca_crt_file }}"
  register: st_out
- fail:
    msg: "Missing {{ gaia_ca_crt_file }} file. Please copy this file from ansible-bastion or make the keys directory a symlink"
  when: st_out.stat.exists != True
- shell: "openssl enc -base64 -A -in {{ gaia_ca_crt_file }}"
  register: gaia_ca_crt_content_out
- set_fact:
    gaia_ca_crt_content: "{{ gaia_ca_crt_content_out.stdout }}"
# gaia CA private key content
- set_fact:
    gaia_ca_key_file: "keys/{{ ec2_region }}/{{ gaia_ca_file }}.key"
- stat:
    path: "{{ gaia_ca_key_file }}"
  register: st_out
- fail:
    msg: "Missing {{ gaia_ca_key_file }} file. Please copy this file from ansible-bastion or make the keys directory a symlink"
  when: st_out.stat.exists != True
- shell: "openssl enc -base64 -A -in {{ gaia_ca_key_file }}"
  register: gaia_ca_key_content_out
- set_fact:
    gaia_ca_key_content: "{{ gaia_ca_key_content_out.stdout }}"

# gaia-pubring.gpg content
- set_fact:
    gaia_pubring_file: "keys/gaia-pubring.gpg"
- stat:
    path: "{{ gaia_pubring_file }}"
  register: st_out
- fail:
    msg: "Missing {{ gaia_pubring_file }} file"
  when: st_out.stat.exists != True
- shell: "openssl enc -base64 -A -in {{ gaia_pubring_file }}"
  register: gaia_pubring_content_out
- set_fact:
    gaia_pubring_content: "{{ gaia_pubring_content_out.stdout }}"

# gaia-secring.gpg content
- set_fact:
    gaia_secring_file: "keys/gaia-secring.gpg"
- stat:
    path: "{{ gaia_secring_file }}"
  register: st_out
- fail:
    msg: "Missing {{ gaia_secring_file }} file"
  when: st_out.stat.exists != True
- shell: "openssl enc -base64 -A -in {{ gaia_secring_file }}"
  register: gaia_secring_content_out
- set_fact:
    gaia_secring_content: "{{ gaia_secring_content_out.stdout }}"

# get current account id
- name: get user details
  local_action: command aws iam get-user --output json
  register: myuser
- set_fact:
    myuser_out: "{{ myuser.stdout }}"
- debug: msg="Working on account {{ myuser_out.User.Arn[13:25] }} ..."

# try acquiring acm certificate (currently only exist for us-east-1)
- name: list acm certificates for our domain
  shell: aws acm list-certificates --certificate-statuses ISSUED
  ignore_errors: yes
  register: acm_result
- set_fact:
    certificate_summary_list: "{{ acm_result.stdout }}"
- name: set acm certificate var
  set_fact:
    certificate_arn: "{{ certificate_summary_list.CertificateSummaryList | selectattr('DomainName','equalto', 'gaiahub.io') | map(attribute='CertificateArn') | list | first }}"
  when: "acm_result.rc == 0"

- debug: msg="There is no issued certificate in aws acm, please check if certificate is expired and should be renewd."
  when: "acm_result.rc != 0"

- name: check if self-signed certificate for ELB already exists
  local_action: command aws iam get-server-certificate --server-certificate-name ELB_Certificate.{{ ec2_region }}
  ignore_errors: yes
  register: cert
  when: "acm_result.rc != 0"

- name: create self-signed certificate for ELB
  local_action: command openssl req -new -nodes -x509 -subj "/C=US/ST=California/L=Palo Alto/O=HP Enterprise/CN=*.{{ ec2_region }}.elb.amazonaws.com" -days 3650 -keyout keys/{{ ec2_region }}/elb.{{ ec2_region }}.key -out keys/{{ ec2_region }}/elb.{{ ec2_region }}.crt -extensions v3_ca creates=keys/{{ ec2_region }}/elb.{{ ec2_region }}.crt
  when: "cert is defined and cert.rc != 0"

- name: upload self-signed certificate for ELB
  local_action: command aws iam upload-server-certificate --server-certificate-name ELB_Certificate.{{ ec2_region }} --certificate-body file://keys/{{ ec2_region }}/elb.{{ ec2_region }}.crt --private-key file://keys/{{ ec2_region }}/elb.{{ ec2_region }}.key
  when: "cert is defined and cert.rc != 0"

- name: set self-signed ssl certificate var if acm certificate not defined
  set_fact:
    certificate_arn: "arn:aws:iam::{{ myuser_out.User.Arn[13:25] }}:server-certificate/ELB_Certificate.{{ ec2_region }}"
  when: certificate_arn is not defined

- name: create security group for ELB
  ec2_group:
    name: "{{ elb_security_group.name }}"
    description: "{{ elb_security_group.desc }}"
    vpc_id: "{{ vpc_id }}"
    region: "{{ ec2_region }}"
    rules: "{{ elb_security_group.rules }}"
  register: elb_sg

- name: Create the security group for the CoreOS cluster
  ec2_group:
    name: "{{ coreos_security_group.name }}"
    description: "{{ coreos_security_group.desc }}"
    vpc_id: "{{ vpc_id }}"
    region: "{{ ec2_region }}"
    rules: "{{ coreos_security_group.rules }}"
  register: coreos_sg

- debug: var=coreos_sg

# Add access to Postgres from coreos_security_group
- name: find rds_sg
  shell: "aws ec2 describe-security-groups --filters Name=group-name,Values='rds_sg_{{ env_nick[environ] }}'"
  register: rds_sg_out
- set_fact:
    rds_sg_out_stdout: "{{ rds_sg_out.stdout | from_json }}"
- debug: var=rds_sg_out_stdout.SecurityGroups[0].GroupId
- name: add rule
  shell: "aws ec2 authorize-security-group-ingress --group-id {{ rds_sg_out_stdout.SecurityGroups[0].GroupId }} --protocol tcp --port 5432 --source-group {{ coreos_sg.group_id }}"

# generate application security keys for further usage by Dex - should be in environment
- name: generate dex admin api key
  shell: "dd if=/dev/random bs=1 count=128 2>/dev/null | base64 | tr -d '\n'"
  register: dex_admin_key_out
- set_fact:
    DEX_API_SECRET: "{{ dex_admin_key_out.stdout }}"
- name: generate dex secret key
  shell: "dd if=/dev/random bs=1 count=32 2>/dev/null | base64 | tr -d '\n'"
  register: dex_secret_key_out
- set_fact:
    DEX_SECRET: "{{ dex_secret_key_out.stdout }}"
- debug: var="{{ DEX_API_SECRET }}"
- debug: var="{{ DEX_SECRET }}"

# generate user-data from template
- name: create template
  template: src='user-data.j2' dest='/tmp/user-data.txt'
- name: load user-data content
  set_fact:
    # user_data: "{{ lookup('file', '/tmp/user-data.txt') | b64encode }}" <-- base64 encoded user-data is not recognized by CoreOS
    user_data: "{{ lookup('file', '/tmp/user-data.txt') }}"

# search for latest CoreOS AMI from alpha/beta/stable channel
- name: search for the latest CoreOS AMI image from "{{ coreos_channel }}"
  ec2_ami_find:
    region: "{{ ec2_region }}"
    name: "CoreOS-{{coreos_channel}}-{{coreos_version}}-hvm"
    virtualization_type: hvm
    sort: name
    sort_order: descending
    sort_end: 1
    no_result_action: fail
  register: find_out
- name: get CoreOS AMI
  set_fact:
    coreos_ami: "{{ find_out.results[0] }}"

- name: create ELB
  ec2_elb_lb:
    name: "{{ coreos_elb_name }}"
    state: present
    connection_draining_timeout: 60
    cross_az_load_balancing: "yes"
    region: "{{ ec2_region }}"
    security_group_ids: "{{ elb_sg.group_id }}"
    subnets: "{{ vpc_public_subnets }}"
    listeners:
      - protocol: http
        load_balancer_port: 80
        instance_port: 80
      - protocol: http
        load_balancer_port: 88
        instance_port: 88
      - protocol: https
        load_balancer_port: 443
        instance_protocol: http
        instance_port: 80
        ssl_certificate_id: "{{ certificate_arn }}"
      - protocol: https
        load_balancer_port: 444
        instance_protocol: http
        instance_port: 88
        ssl_certificate_id: "{{ certificate_arn }}"
      - protocol: http
        load_balancer_port: 1936
        instance_port: 1936
      - protocol: http
        load_balancer_port: 8083
        instance_port: 8083
      - protocol: http
        load_balancer_port: 8086
        instance_port: 8086
    health_check:
      ping_protocol: http
      ping_port: 88
      ping_path: "/mgs/"
      response_timeout: 5
      interval: 10
      unhealthy_threshold: 4
      healthy_threshold: 5

- name: get ELB details
  local_action: command aws elb describe-load-balancers --load-balancer-names {{ coreos_elb_name }} --max-items 1 --output json
  register: elb_fact_out

- name: prepare ELB fact
  set_fact:
    elb_fact: "{{ elb_fact_out.stdout }}"

- name: obtain alias hosted zone id
  set_fact:
    id: "{{ hz_ids | get_hosted_zone_id() }}"

- name: create route53 subdomain for environment
  route53:
    command: create
    zone:  gaiahub.io
    record: "{{ dns }}.gaiahub.io"
    type: A
    alias: True
    overwrite: True
    alias_hosted_zone_id: "{{ id }}"
    value: "dualstack.{{ elb_fact.LoadBalancerDescriptions[0].CanonicalHostedZoneName }}"

- name: create route53 4th level domain for webhooks
  route53:
    command: create
    zone:  gaiahub.io
    record: "webhook.{{ dns }}.gaiahub.io"
    type: A
    alias: True
    overwrite: True
    alias_hosted_zone_id: "{{ id }}"
    value: "dualstack.{{ elb_fact.LoadBalancerDescriptions[0].CanonicalHostedZoneName }}"

# using JSON file as a workaround for Ansible bug: https://github.com/ansible/ansible/issues/9362
- name: create coreos volume json template
  template: src='volume.j2' dest='/tmp/coreos_volume.json'

# create CoreOS launch configuration
# note: for flannel use manually create role with required pilicies
#       https://coreos.com/flannel/docs/latest/vpc-backend.html
- name: create CoreOS launch configuration
  shell: "aws autoscaling create-launch-configuration --launch-configuration-name '{{ coreos_lc_name }}' --key-name '{{ coreos_keypair_name }}' --image-id '{{ coreos_ami.ami_id }}' --security-groups '{{ coreos_sg.group_id }}' --instance-type '{{ coreos_instance_type }}' --iam-instance-profile '{{ coreos_instance_profile }}' --user-data 'file:///tmp/user-data.txt' --block-device-mappings 'file:///tmp/coreos_volume.json'"

# create CoreOS autoscale group
- name: create CoreOS autoscale group
  ec2_asg:
    name: "{{ coreos_asg_name }}"
    load_balancers:
      - "{{ coreos_elb_name }}"
    region: "{{ ec2_region }}"
    launch_config_name: "{{ coreos_lc_name }}"
    health_check_period: "{{ coreos_health_check_period }}"
    desired_capacity: "{{ coreos_cluster_size[environ] }}"
    min_size: "{{ coreos_cluster_size[environ] }}"
    max_size: "{{ coreos_max_cluster_size }}"
    tags: "{{ coreos_instance_tags }}"
    vpc_zone_identifier: "{{ vpc_private_subnets }}"
    wait_for_instances: yes
  register: asg_out

- name: Turn off "source destination check" - needed for flunnel
  local_action: command aws ec2 modify-instance-attribute --region {{ ec2_region }} --instance-id {{ item }} --no-source-dest-check
  with_items: "{{ asg_out.instances }}"

# Check if DB is available
- name: check Postgres availability
  rds:
    command: facts
    instance_name: "{{ postgres_name }}"
    region: "{{ ec2_region }}"
  register: dbf
  until: dbf.instance.status == "available"
  retries: 15
  delay: 60
