- debug: var=vpc_id

# set facts with file content for cloud-config
- set_fact:
    gaia_ca_crt_file: "keys/{{ ec2_region }}/{{ gaia_ca_file }}.crt"
- stat:
    path: "{{ gaia_ca_crt_file }}"
  register: st_out
- fail:
    msg: "Missing {{ gaia_ca_crt_file }} file. Please copy this file from ansible-bastion or make the keys directory a symlink"
  when: st_out.stat.exists != True
- shell: "base64 -w0 {{ gaia_ca_crt_file }}"
  register: gaia_ca_crt_content_out
- set_fact:
    gaia_ca_crt_content: "{{ gaia_ca_crt_content_out.stdout }}"

# get current account id
- name: get  user details
  local_action: command aws iam get-user --output json
  register: myuser
- set_fact:
    myuser_out: "{{ myuser.stdout }}"
- debug: msg="Working on account {{ myuser_out.User.Arn[13:25] }} ..."

- name: check if self-signed certificate for ELB already exists
  local_action: command aws iam get-server-certificate --server-certificate-name ELB_Certificate.{{ ec2_region }}
  ignore_errors: yes
  register: cert

- name: create self-signed certificate for ELB
  local_action: command openssl req -new -nodes -x509 -subj "/C=US/ST=California/L=Palo Alto/O=HP Enterprise/CN=*.{{ ec2_region }}.elb.amazonaws.com" -days 3650 -keyout keys/{{ ec2_region }}/elb.{{ ec2_region }}.key -out keys/{{ ec2_region }}/elb.{{ ec2_region }}.crt -extensions v3_ca creates=keys/{{ ec2_region }}/elb.{{ ec2_region }}.crt
  when: "cert.rc != 0"

- name: upload self-signed certificate for ELB
  local_action: command aws iam upload-server-certificate --server-certificate-name ELB_Certificate.{{ ec2_region }} --certificate-body file://keys/{{ ec2_region }}/elb.{{ ec2_region }}.crt --private-key file://keys/{{ ec2_region }}/elb.{{ ec2_region }}.key
  when: "cert.rc != 0"

- name: create security group for ELB
  ec2_group:
    name: "{{ elb_security_group.name }}"
    description: "{{ elb_security_group.desc }}"
    vpc_id: "{{ vpc_id }}"
    region: "{{ ec2_region }}"
    rules: "{{ elb_security_group.rules }}"
  register: elb_sg

- name: Create the security group for the CoreOS cluster
  ec2_group:
    name: "{{ coreos_security_group.name }}"
    description: "{{ coreos_security_group.desc }}"
    vpc_id: "{{ vpc_id }}"
    region: "{{ ec2_region }}"
    rules: "{{ coreos_security_group.rules }}"
  register: coreos_sg

# generate user-data from template
- name: create template
  template: src='user-data.j2' dest='/tmp/user-data.txt'
- name: load user-data content
  set_fact:
    # user_data: "{{ lookup('file', '/tmp/user-data.txt') | b64encode }}" <-- base64 encoded user-data is not recognized by CoreOS
    user_data: "{{ lookup('file', '/tmp/user-data.txt') }}"

# search for latest CoreOS AMI from alpha/beta/stable channel
- name: search for the latest CoreOS AMI image from "{{ coreos_channel }}"
  ec2_ami_find:
    region: "{{ ec2_region }}"
    name: "CoreOS-{{coreos_channel}}-{{coreos_version}}-hvm"
    virtualization_type: hvm
    sort: name
    sort_order: descending
    sort_end: 1
    no_result_action: fail
  register: find_out
- name: get CoreOS AMI
  set_fact:
    coreos_ami: "{{ find_out.results[0] }}"

- name: create ELB
  ec2_elb_lb:
    name: "{{ coreos_elb_name }}"
    state: present
    connection_draining_timeout: 60
    cross_az_load_balancing: "yes"
    region: "{{ ec2_region }}"
    security_group_ids: "{{ elb_sg.group_id }}"
    subnets: "{{ vpc_public_subnets }}"
    listeners:
      - protocol: http
        load_balancer_port: 80
        instance_port: 80
      - protocol: http
        load_balancer_port: 88
        instance_port: 88
      - protocol: https
        load_balancer_port: 443
        instance_protocol: http
        instance_port: 80
        ssl_certificate_id: "arn:aws:iam::{{ myuser_out.User.Arn[13:25] }}:server-certificate/ELB_Certificate.{{ ec2_region }}"
      - protocol: https
        load_balancer_port: 444
        instance_protocol: http
        instance_port: 88
        ssl_certificate_id: "arn:aws:iam::{{ myuser_out.User.Arn[13:25] }}:server-certificate/ELB_Certificate.{{ ec2_region }}"
      - protocol: http
        load_balancer_port: 1936
        instance_port: 1936
      - protocol: http
        load_balancer_port: 8083
        instance_port: 8083
      - protocol: http
        load_balancer_port: 8086
        instance_port: 8086
    health_check:
      ping_protocol: http
      ping_port: 88
      ping_path: "/mgs/"
      response_timeout: 5
      interval: 10
      unhealthy_threshold: 4
      healthy_threshold: 5

- name: get ELB details
  local_action: command aws elb describe-load-balancers --load-balancer-names coreos-elb-dev --max-items 1 --output json
  register: elb_fact_out

- name: prepare ELB fact
  set_fact:
    elb_fact: "{{ elb_fact_out.stdout }}" 

- name: obtain alias hosted zone id
  set_fact:
    id: "{{ hz_ids | get_hosted_zone_id() }}"

- name: create route53 subdomain for environment
  route53:
    command: create
    zone:  gaiahub.io
    record: "{{ dns }}.gaiahub.io"
    type: A
    alias: True
    alias_hosted_zone_id: "{{ id }}"
    value: "dualstack.{{ elb_fact.LoadBalancerDescriptions[0].CanonicalHostedZoneName }}"


# create CoreOS launch configuration
# note: for flannel use manually create role with required pilicies
#       https://coreos.com/flannel/docs/latest/vpc-backend.html
- name: create CoreOS launch configuration
  shell: "aws autoscaling create-launch-configuration --launch-configuration-name '{{ coreos_lc_name }}' --key-name '{{ coreos_keypair_name }}' --image-id '{{ coreos_ami.ami_id }}' --security-groups '{{ coreos_sg.group_id }}' --instance-type '{{ coreos_instance_type }}' --iam-instance-profile '{{ coreos_instance_profile }}' --user-data 'file:///tmp/user-data.txt' --block-device-mappings '{{ coreos_block_device_mappings | to_json }}'"

# create CoreOS autoscale group
- name: create CoreOS autoscale group
  ec2_asg:
    name: "{{ coreos_asg_name }}"
    load_balancers:
      - "{{ coreos_elb_name }}"
    region: "{{ ec2_region }}"
    launch_config_name: "{{ coreos_lc_name }}"
    health_check_period: "{{ coreos_health_check_period }}"
    desired_capacity: "{{ coreos_cluster_size }}"
    min_size: "{{ coreos_cluster_size }}"
    max_size: "{{ coreos_max_cluster_size }}"
    tags: "{{ coreos_instance_tags }}"
    vpc_zone_identifier: "{{ vpc_private_subnets }}"
    wait_for_instances: yes
  register: asg_out

- name: Turn off "source destination check" - needed for flunnel
  local_action: command aws ec2 modify-instance-attribute --region {{ ec2_region }} --instance-id {{ item }} --no-source-dest-check
  with_items: "{{ asg_out.instances }}"

# create volumes for coreos cluster

# create influxdb volume
- name: Create influxdb volume (non io1)
  shell: "aws ec2 create-volume --size {{ influxdb_volume.size }} --availability-zone '{{ ec2_region_az1 }}' --volume-type {{ influxdb_volume.type }} --output json"
  register: influxdb_volume_out
  when: influxdb_volume.type != "io1"
- set_fact:
    influxdb_volume_info: "{{ influxdb_volume_out.stdout | from_json }}"
  when: influxdb_volume.type != "io1"

- name: Create influxdb volume (io1)
  shell: "aws ec2 create-volume --size {{ influxdb_volume.size }} --availability-zone '{{ ec2_region_az1 }}' --volume-type {{ influxdb_volume.type }} --iops {{ influxdb_volume.iops }} --output json"
  register: influxdb_volume_out
  when: influxdb_volume.type == "io1"
- set_fact:
    influxdb_volume_info: "{{ influxdb_volume_out.stdout | from_json }}"
  when: influxdb_volume.type == "io1"

- name: Tag influxdb volume
  ec2_tag:
    region: "{{ ec2_region }}"
    resource: "{{ influxdb_volume_info.VolumeId }}"
    state: present
    tags:
      Name: "{{ influxdb_volume.name }}"

# create rabbitmq_master volume
- name: Create rabbitmq_master volume (non io1)
  shell: "aws ec2 create-volume --size {{ rabbitmq_master_volume.size }} --availability-zone '{{ ec2_region_az1 }}' --volume-type {{ rabbitmq_master_volume.type }} --output json"
  register: rabbitmq_master_volume_out
  when: rabbitmq_master_volume.type != "io1"
- set_fact:
    rabbitmq_master_volume_info: "{{ rabbitmq_master_volume_out.stdout | from_json }}"
  when: rabbitmq_master_volume.type != "io1"

- name: Create rabbitmq_master volume (io1)
  shell: "aws ec2 create-volume --size {{ rabbitmq_master_volume.size }} --availability-zone '{{ ec2_region_az1 }}' --volume-type {{ rabbitmq_master_volume.type }} --iops {{ rabbitmq_master_volume.iops }} --output json"
  register: rabbitmq_master_volume_out
  when: rabbitmq_master_volume.type == "io1"
- set_fact:
    rabbitmq_master_volume_info: "{{ rabbitmq_master_volume_out.stdout | from_json }}"
  when: rabbitmq_master_volume.type == "io1"

- name: Tag rabbitmq_master volume
  ec2_tag:
    region: "{{ ec2_region }}"
    resource: "{{ rabbitmq_master_volume_info.VolumeId }}"
    state: present
    tags:
      Name: "{{ rabbitmq_master_volume.name }}"
